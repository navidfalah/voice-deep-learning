{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karoldvl/ESC-50"
      ],
      "metadata": {
        "id": "LUJOtnCrETA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32944b2-eb0d-4f56-bb1d-149358e55f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ESC-50' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "\n",
        "display.Audio('ESC-50/audio/1-100032-A-0.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "C6u9mKuCYwt_",
        "outputId": "faeb6b4d-2e89-4d54-ed0c-e4da01f132ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6e8626f970ce>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ESC-50/audio/1-100032-A-0.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "esc50_list = [f.split(\"-\")[-1].replace(\".wav\", \"\") for f in glob.glob(\"ESC-50/audio/*.wav\")]\n",
        "print(esc50_list)\n",
        "print(Counter(esc50_list))"
      ],
      "metadata": {
        "id": "fIdxTMReZ3Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "id": "DIX5DFE5b9Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install sox"
      ],
      "metadata": {
        "id": "LAwhTI0Xd9nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchaudio"
      ],
      "metadata": {
        "id": "xXiBOSQseC4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio.transforms as T\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ESC50(Dataset):\n",
        "\n",
        "    def __init__(self, path, transform=None):\n",
        "        files = Path(path).glob('*.wav')\n",
        "        self.items = [(f, int(f.name.split(\"-\")[-1].replace(\".wav\", \"\"))) for f in files]\n",
        "        self.length = len(self.items)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename, label = self.items[index]\n",
        "        audio, sample_rate = torchaudio.load(filename)\n",
        "\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "\n",
        "        return audio, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "# Spectrogram transformation setup\n",
        "spectrogram_transform = T.Spectrogram(\n",
        "    n_fft=512,\n",
        "    hop_length=256,\n",
        "    power=2\n",
        ")"
      ],
      "metadata": {
        "id": "yIOmVnlJebfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/ESC-50/train /content/ESC-50/valid /content/ESC-50/test"
      ],
      "metadata": {
        "id": "FCa3v0C8vkx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/ESC-50/audio/1* /content/ESC-50/train\n",
        "! mv /content/ESC-50/audio/2* /content/ESC-50/train\n",
        "! mv /content/ESC-50/audio/3* /content/ESC-50/train\n",
        "! mv /content/ESC-50/audio/4* /content/ESC-50/valid\n",
        "! mv /content/ESC-50/audio/5* /content/ESC-50/test"
      ],
      "metadata": {
        "id": "pihu32cxiBxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### get the data from the path and create a custom dataset for each\n",
        "test_esc50 = \"/content/ESC-50/test\"\n",
        "train_esc50 = \"/content/ESC-50/train\"\n",
        "valid_esc50 = \"/content/ESC-50/valid\"\n",
        "\n",
        "test_dataset = ESC50(path=test_esc50, transform=spectrogram_transform)\n",
        "train_dataset = ESC50(path=train_esc50, transform=spectrogram_transform)\n",
        "valid_dataset = ESC50(path=valid_esc50, transform=spectrogram_transform)\n",
        "\n",
        "### create the data loader for each of them\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "8n2Tp3iaUX1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_loader))\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))"
      ],
      "metadata": {
        "id": "vAiTn76GUu7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset[0])\n",
        "audio, label = test_dataset[0]\n",
        "print(audio.shape)\n",
        "print(label)"
      ],
      "metadata": {
        "id": "yi0c292MUyz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# Spectrogram transformation setup\n",
        "spectrogram_transform = T.Spectrogram(\n",
        "    n_fft=2048,\n",
        "    hop_length=512,\n",
        "    power=2  # Calculate the power spectrogram\n",
        ")\n",
        "\n",
        "# Function to plot waveform and spectrogram\n",
        "def plot_waveform_and_spectrogram(audio, label):\n",
        "    \"\"\"\n",
        "    Plots both the waveform and its corresponding spectrogram.\n",
        "\n",
        "    Args:\n",
        "        audio (torch.Tensor): Input audio tensor (waveform or spectrogram).\n",
        "        label (str/int): Label associated with the audio.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    if audio.dim() == 2 and audio.size(0) == 1:  # Single-channel waveform [1, time]\n",
        "        waveform = audio.squeeze(0).numpy()\n",
        "\n",
        "        # Plot waveform\n",
        "        axs[0].plot(waveform)\n",
        "        axs[0].set_title(f\"Waveform - Label: {label}\")\n",
        "        axs[0].set_xlabel(\"Time (samples)\")\n",
        "        axs[0].set_ylabel(\"Amplitude\")\n",
        "\n",
        "        # Generate spectrogram\n",
        "        spectrogram = spectrogram_transform(audio)\n",
        "    elif audio.dim() == 3 and audio.size(0) == 1:  # Single-channel spectrogram [1, freq, time]\n",
        "        spectrogram = audio.squeeze(0)\n",
        "\n",
        "        # Remove the waveform plot since the input is already a spectrogram\n",
        "        fig.delaxes(axs[0])\n",
        "        axs = [None, axs[1]]\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected audio tensor shape: {audio.shape}\")\n",
        "\n",
        "    # Plot spectrogram\n",
        "    im = axs[1].imshow(\n",
        "        spectrogram.log2().detach().numpy(),\n",
        "        cmap='viridis',\n",
        "        aspect='auto',\n",
        "        origin='lower'\n",
        "    )\n",
        "    fig.colorbar(im, ax=axs[1])\n",
        "    axs[1].set_title(f\"Spectrogram - Label: {label}\")\n",
        "    axs[1].set_xlabel(\"Time Bins\")\n",
        "    axs[1].set_ylabel(\"Frequency Bins\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Loop through the first three samples in the test dataset\n",
        "for i in range(3):\n",
        "    audio, label = test_dataset[i]\n",
        "    print(f\"Processing Sample {i+1}: Audio Shape {audio.shape}, Label: {label}\")\n",
        "    plot_waveform_and_spectrogram(audio, label)"
      ],
      "metadata": {
        "id": "VaVk4UHeOi8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Halves the dimensions at each pooling layer\n",
        "\n",
        "        # Dynamically calculate the input size for the fully connected layer\n",
        "        self.fc1_input_size = self._get_conv_output_size((1, 257, 862))\n",
        "        self.fc1 = nn.Linear(self.fc1_input_size, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)  # Adjust number of classes if needed\n",
        "\n",
        "    def _get_conv_output_size(self, shape):\n",
        "        \"\"\"\n",
        "        Pass a dummy tensor through conv layers to calculate the flattened output size.\n",
        "        \"\"\"\n",
        "        dummy_input = torch.rand(1, *shape)  # Create a dummy tensor with input shape\n",
        "        dummy_output = self.pool(F.relu(self.conv1(dummy_input)))\n",
        "        dummy_output = self.pool(F.relu(self.conv2(dummy_output)))\n",
        "        dummy_output = self.pool(F.relu(self.conv3(dummy_output)))\n",
        "        return int(torch.numel(dummy_output))  # Total number of features after flattening\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # First convolution + pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Second convolution + pooling\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # Third convolution + pooling\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer 1\n",
        "        x = self.fc2(x)  # Fully connected layer 2 (output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VXRiXiqkQoRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = AudioClassifier()\n",
        "\n",
        "# Test with dummy input\n",
        "dummy_input = torch.rand(32, 1, 257, 862)  # Batch size = 64\n",
        "output = model(dummy_input)\n",
        "print(f\"Model output shape: {output.shape}\")  # Should be (64, 10)"
      ],
      "metadata": {
        "id": "XJOC6noBdGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "import torch\n",
        "\n",
        "# Define the model, loss criterion, optimizer, and set hyperparameters\n",
        "model = AudioClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 10\n",
        "best_lr = 0.01  # Adjust based on learning rate finder results\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(data)  # Compute model output\n",
        "        loss = criterion(output, target)  # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model weights\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print average training loss per epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for data, target in valid_loader:\n",
        "            output = model(data)  # Compute model output\n",
        "            loss = criterion(output, target)  # Calculate loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    # Print average validation loss per epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {total_val_loss / len(valid_loader)}')"
      ],
      "metadata": {
        "id": "FFzP5qrQQtRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}